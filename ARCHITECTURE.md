# Архитектура TR181 Cloud Platform

## Обзор

Платформа состоит из микросервисной архитектуры с разделением ответственности между компонентами.

## Компоненты

### 1. API Gateway (`services/api-gateway`)
- **Назначение**: Единая точка входа для всех API запросов
- **HTTP эндпоинты** (порт 8080):
  - `GET /api/v1/metric/{metric-type}` - получение метрик
  - `GET /api/v1/alert/{alert-type}` - получение статистики алертов
- **gRPC эндпоинты** (порт 9090):
  - `tr181.api.TR181Api/GetMetric` - получение метрик
  - `tr181.api.TR181Api/GetAlert` - получение статистики алертов
- **Особенности**:
  - Кэширование запросов в Redis (TTL 30 секунд)
  - CPU нагрузка для автоскейлинга
  - Response time < 1 секунды благодаря кэшированию

### 2. Data Ingestion Service (`services/data-ingestion`)
- **Назначение**: Прием и сохранение TR181 данных
- **Ответственность**: только парсинг сообщений и запись метрик в БД
- **Структура**: config, parser, storage, handler — разделение ответственности
- **Не знает** об алертах

### 3. Alert Processor (`services/alert-processor`)
- **Назначение**: Оценка условий и создание алертов
- **Источник**: тот же topic `tr181-device-data` (своя подписка)
- **Адаптеры**: один файл на тип алерта (cpu_adapter, wifi_adapter)
- **Логика**: каждый адаптер получает полный payload, сам решает, нужен ли алерт

### 4. Simulator (`simulator`)
- **Назначение**: Симуляция 20,000 устройств
- **Особенности**:
  - Публикует TR181 данные в Apache Pulsar topic `tr181-device-data`
  - Каждое устройство отправляет данные каждые 30 секунд
  - Генерация реалистичных данных с вариациями
  - Периодическая генерация условий для алертов (5% CPU, 3% WiFi)
  - Батчинг сообщений для эффективной отправки

## Технологический стек

### Транспорт и протоколы
- **Внешний API**: HTTP REST (Gin framework) и gRPC (порт 9090)
- **Прием данных**: Apache Pulsar (topic `tr181-device-data`)
- **Межсервисное взаимодействие**: Apache Pulsar (message broker)
- **TR181 формат**: JSON с поддержкой customer extensions

### Базы данных
- **PostgreSQL + TimescaleDB**: 
  - Хранение временных рядов метрик
  - Хранение алертов
  - Hypertables для оптимизации запросов по времени
  - Индексы для быстрого поиска по serial_number и времени

- **Redis**:
  - Кэширование результатов запросов метрик
  - Кэширование статистики алертов
  - TTL 30 секунд для баланса между актуальностью и производительностью

### Инфраструктура
- **Docker Compose**: Оркестрация инфраструктурных сервисов
- **Apache Pulsar**: Message broker для TR181 данных и алертов

## Архитектурные решения

### 1. Микросервисная архитектура
- Разделение ответственности между сервисами
- Независимое масштабирование каждого компонента
- Возможность горизонтального масштабирования

### 2. Асинхронная обработка алертов
- Алерты обрабатываются асинхронно через Apache Pulsar
- Допускается задержка обработки (non-blocking)
- Гарантированная доставка через персистентные topics Pulsar

### 3. Кэширование
- Redis кэш для часто запрашиваемых данных
- Снижение нагрузки на PostgreSQL
- Обеспечение response time < 1 секунды

### 4. Оптимизация БД
- TimescaleDB для эффективной работы с временными рядами
- Индексы на часто используемых полях
- Партиционирование по времени (hypertables)

### 5. CPU нагрузка для автоскейлинга
- Каждый сервис создает контролируемую CPU нагрузку
- Позволяет системам автоскейлинга (Kubernetes) правильно масштабировать сервисы
- Нагрузка имитирует реальную работу сервиса

## Производительность

### Требования
- **Response time**: < 1 секунды (достигается через кэширование)
- **Нагрузка**: 20,000 активных устройств
- **Частота отправки**: каждые 30 секунд
- **Память**: оптимизировано для машины с 16GB RAM

### Оптимизации
1. **Кэширование**: Redis кэш снижает нагрузку на БД
2. **Батчинг**: Симулятор ограничивает конкурентность
3. **Индексы**: Оптимизированные индексы в PostgreSQL
4. **Connection pooling**: Настроенные пулы соединений

## Масштабируемость

### Горизонтальное масштабирование
- Все сервисы stateless и могут масштабироваться горизонтально
- API Gateway может быть за балансировщиком нагрузки
- Data Ingestion может обрабатывать больше устройств при масштабировании
- Alert Processor может масштабироваться для обработки большего количества алертов

### Вертикальное масштабирование
- PostgreSQL может быть настроен с большим количеством ресурсов
- Redis может быть настроен с большим объемом памяти для кэша

## Мониторинг и отладка

### Health checks
- Каждый сервис предоставляет `/health` эндпоинт
- Docker health checks для инфраструктурных сервисов

### Логирование
- Все сервисы логируют важные события
- Ошибки логируются с контекстом

## Безопасность

### Рекомендации для production
1. Добавить аутентификацию и авторизацию
2. Использовать TLS для всех соединений
3. Настроить firewall правила
4. Регулярно обновлять зависимости
5. Использовать secrets management для паролей

## Будущие улучшения

1. **Метрики и мониторинг**: Prometheus + Grafana
2. **Трейсинг**: OpenTelemetry для распределенной трассировки
3. **Rate limiting**: Ограничение запросов на API
4. **Compression**: Сжатие данных при передаче
5. **Replication**: Репликация БД для высокой доступности
