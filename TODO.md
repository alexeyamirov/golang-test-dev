# TR181 Cloud Platform — как работает приложение и идеи по улучшению

## Как сейчас работает приложение

### Общая архитектура

Платформа состоит из **микросервисов на Go** и **инфраструктуры в Docker**:

- **Инфраструктура (docker-compose):**
  - **PostgreSQL (TimescaleDB)** — хранение метрик и алертов, гибридные таблицы с партиционированием по времени.
  - **Redis** — кэш ответов API (метрики и статистика алертов), TTL 30 секунд.
  - **Apache Pulsar** — шина сообщений: один топик с данными устройств (`tr181-device-data`), один топик логов (`tr181-logs`).

- **Сервисы (запускаются отдельно, например через `start-all.ps1` / `scripts/run-all.sh`):**
  1. **simulator** — симулирует 20 000 устройств; каждые 30 секунд генерирует TR181-данные, собирает в батчи по 50 и публикует в Pulsar в топик `tr181-device-data`.
  2. **data-ingestion** — подписывается на `tr181-device-data` (Shared), парсит JSON в модель TR181, сохраняет метрики в PostgreSQL (все типы: CPU, память, температура, WiFi, Ethernet, uptime).
  3. **alert-processor** — подписывается на тот же топик `tr181-device-data` (Shared), парсит сообщения, прогоняет данные через адаптеры (CPU, WiFi): при превышении порогов создаёт алерты (warning/error) и сохраняет их в PostgreSQL.
  4. **api-gateway** — HTTP (порт 8080) и gRPC (порт 9090): отдаёт метрики и статистику алертов по устройству и периоду; сначала проверяет Redis, при промахе идёт в PostgreSQL и кэширует результат.
  5. **log-viewer** (опционально) — подписывается на топик `tr181-logs`, выводит логи всех сервисов в один терминал с цветом по уровню (info/warn/error) и задержкой между строками (`LOG_VIEWER_DELAY_MS`).

### Поток данных

1. **Simulator** → публикует в Pulsar топик `tr181-device-data` (JSON: `TR181Device` с серийным номером, временем и `DeviceData`).
2. **data-ingestion** и **alert-processor** независимо читают из этого топика (каждый со своей подпиской), обрабатывают одни и те же сообщения параллельно.
3. **data-ingestion** пишет строки в таблицы метрик (TimescaleDB).
4. **alert-processor** по правилам адаптеров (CPU ≥60% / ≥80%, WiFi слабее порогов) пишет алерты в таблицу алертов.
5. **api-gateway** по запросу читает метрики/алерты из БД (с кэшем в Redis) и отдаёт по HTTP/gRPC.
6. Все сервисы при наличии `PULSAR_URL` могут писать логи в топик `tr181-logs`; **log-viewer** их отображает.

### Модель данных (TR181)

- **TR181Device**: `serial_number`, `timestamp`, `data` (DeviceData).
- **DeviceData**: CPU/Memory %, температуры (CPU/Board/Radio), уровни сигнала WiFi 2.4/5/6 GHz (dBm), Ethernet bytes sent/received, uptime.
- **Метрики** в API: типы вроде `cpu-usage`, `memory-usage`, `wifi-2ghz-signal` и т.д.
- **Алерты**: `high-cpu-usage`, `low-wifi` с уровнями warning/error.

### Конфигурация

- Всё через переменные окружения: `POSTGRES_CONN_STR`, `REDIS_ADDR`, `PULSAR_URL`, `PORT`, `GRPC_PORT`, `LOG_VIEWER_DELAY_MS` и т.д.
- Значения по умолчанию зашиты в коде (localhost для Postgres, Redis, Pulsar).

---

## Предложения по улучшению

### Надёжность и эксплуатация

- **Единый конфиг** — вынести все переменные окружения в один конфиг-пакет или YAML/env-файл с валидацией при старте, чтобы не разбрасывать `os.Getenv` по сервисам.
- **Health checks** — добавить HTTP/gRPC health-эндпоинты во все сервисы (проверка Postgres, Redis, Pulsar) и использовать их в оркестрации и мониторинге.
- **Graceful shutdown** — в data-ingestion и alert-processor при получении SIGTERM дожидаться обработки текущего сообщения и только потом завершать; по возможности не терять сообщения при перезапуске.
- **Повторные попытки и dead letter** — при ошибке записи в БД не только логировать, но и Nack с reconsumeLater или отправлять в отдельный топик (DLQ), чтобы не терять события.
- **Метрики Prometheus** — счётчики обработанных сообщений, ошибок, латентность записи в БД; экспорт через `/metrics` для дашбордов.

### Масштабирование и производительность

- **Партиционирование топика Pulsar** — топик `tr181-device-data` партиционировать по `serial_number` (или по его хэшу), чтобы порядок сообщений по устройству сохранялся и нагрузка распределялась по партициям.
- **Батчевая запись в БД** — в data-ingestion накапливать строки в буфер и писать батчами (COPY или batch INSERT), чтобы снизить нагрузку на PostgreSQL при высокой частоте сообщений.
- **Ограничение скорости (backpressure)** — при отставании consumer от producer учитывать задержки Pulsar и при необходимости ограничивать приём или масштабировать воркеры (несколько инстансов data-ingestion/alert-processor с Shared subscription уже поддерживаются).

### Безопасность и API

- **Аутентификация/авторизация** — добавить API-ключи или JWT для HTTP/gRPC, чтобы доступ к метрикам и алертам был только у доверенных клиентов.
- **Лимиты и валидация** — ограничить размер периода запроса (например, не более 30 дней) и максимальное количество точек в ответе, чтобы тяжёлые запросы не перегружали БД и кэш.
- **HTTPS/TLS** — для production включить TLS для API Gateway и по возможности для gRPC.

### Функциональность

- **Фильтрация алертов** — в API добавить фильтры по уровню (warning/error), по типу устройства или по времени «последние N алертов».
- **Агрегации** — опция отдачи метрик с пониженной детализацией (например, среднее за час/день) для длинных периодов без выгрузки сырых точек.
- **Уведомления** — при появлении алерта отправлять событие в отдельный топик или вебхук (email, Slack, Telegram) для операторов.
- **Управление правилами алертов** — вынести пороги (CPU 60%/80%, WiFi dBm) в конфиг или БД, чтобы менять их без пересборки alert-processor.

### Разработка и качество

- **Единый способ конфигурации** — общий пакет `config` с флагами или env для всех сервисов, чтобы не дублировать строки подключения и таймауты.
- **Интеграционные тесты** — тесты с тестовым контейнером Pulsar/Postgres или моками: «simulator отправил сообщение → data-ingestion сохранил метрику → api-gateway её вернул».
- **Структурированное логирование** — переход на логгер с полями (zerolog/zap), чтобы в log-viewer и в системах сбора логов удобно фильтровать по полям.
- **Документация API** — OpenAPI для HTTP и описание gRPC (например, через комментарии в proto), чтобы клиенты могли генерировать клиенты и понимать контракты.

### Инфраструктура

- **Один скрипт/команда для полного стека** — уже есть `start-all.ps1` и `run-all.sh`; при необходимости добавить вариант «инфраструктура + все сервисы в Docker» для быстрого прогона сценариев.
- **Отдельный топик для алертов** — если понадобится реагировать на алерты в реальном времени (например, уведомления), можно снова публиковать события в топик `alerts` и подписываться на него отдельным сервисом, не меняя текущую запись в БД.

---

Итог: приложение уже реализует полный цикл «симуляция → Pulsar → сохранение метрик и алертов → API с кэшем». Предложения выше направлены на устойчивость, масштабируемость, безопасность и удобство сопровождения в учебном или боевом режиме.
